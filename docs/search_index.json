[["index.html", "STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 4 1.1 Classification trees 1.2 Bagging and random forests", " STATS5099 Data Mining and Machine Learning 1 Welcome to DMML Lab 4 In week 4, we have studied three tree-based methods, namely classification trees, bagging and random forests. 1.1 Classification trees Classification trees can be implemented by using the rpart and rpart.plot packages. Some key codes are summarised below. library(rpart); library(rpart.plot) # build a tree for dataset &#39;Data&#39; with features &#39;X1&#39;, &#39;X2&#39; etc and class label &#39;Y&#39; Model &lt;- rpart(Y~X1+X2+..., data=Data, method=&quot;class&quot;) Model # visualise the tree rpart.plot(Model, type=2, extra=4) #check help page for more options about type and extra # make predictions for new data with features &#39;Xnew&#39; Ynew.pred &lt;- predict(Model, newdata=Xnew, type=&quot;class&quot;) One thing to be cautious when building the tree (or any classifier) is to avoid overfitting. This can be achieved by either setting stopping criteria to prevent tree from growing, or pruning a large tree back to simple trees. # set stopping criteria via &#39;control&#39;: smaller values of minsplit, minbucket and cp # and larger value of maxdepth lead to a larger tree Model2 &lt;- rpart(Y~X1+X2+..., data=Data, method=&quot;class&quot;, control=rpart.control(minsplit=20, minbucket=round(minsplit/3), maxdepth = 30, cp=0.01)) #values are set as default # prune the tree printcp(Model) plotcp(Model) 1.2 Bagging and random forests Bagging and random forests can be both implemented by using the randomForest function from the randomForest package. The difference is that in bagging, all features are used to build the tree for each bootstrapped sample, whereas in random forests, only a subset of features are used (set as the square root of all features by default). library(randomForest) # bagging Model &lt;- randomForest(Y~X1+X2+..., data=Data, mtry=n_feature) #n_feature is the number of features in the data # random forests Model &lt;- randomForest(Y~X1+X2+..., data=Data) "],["exercise-1-crabs-data.html", "2 Exercise 1: Crabs Data 2.1 Pruning", " 2 Exercise 1: Crabs Data In the crabs data example we are interested in predicting the sex of a crab based on its morphological measurements. This data is available in the MASS library with the following variables: sp: species - “B” or “O” for blue or orange (not used); sex: as it says (the classification variable of interest); index: index 1:50 within each of the four groups (not used); FL: frontal lobe size (mm); RW: rear width (mm); CL: carapace length (mm); CW: carapace width (mm); BD: body depth (mm). This data can be extracted from the MASS library via: library(MASS) data(crabs) We are interested in predicting the sex of the crab based on the last 5 variables, so we create a data frame of these variables to be used. data.crabs &lt;- as.data.frame(crabs[, c(2, 4:8)]) Task Split the data into training, validation and test data sets (50%, 25%, 25% for each set). Hint Check Lab 3, Section 2.2.1. Build a classification tree on the training data and plot the tree. Note down the decision rules for each node and ensure that you understand what each of the numbers mean. Hint Use the rpart and rpart.plot commands to build and visualise the classification tree. Print out the rules of the classification tree built in step 2. Again, you should understand what each of the numbers corresponds to. Based on the plot or output of the classification tree, predict the class for the following observation. ## FL RW CL CW BD ## 150 23.1 15.7 47.6 52.8 21.6 You can also check the result by using the predict function in R. predict(crabs.rt, newdata=valid.crab[1,-1],type=&quot;class&quot;) How could you determine if the classification tree is under-fitting or overfitting? Hint Which evaluation criteria may be used to assess the performance of classification tree? Calculate sensitivity and specificity for training and validation sets, assuming female is the positive class and male is the negative class. Based on results in (b), what's your conclusion about under-/over-fitting? 2.1 Pruning We may also want to reduce the size of the tree (prune it) to avoid overfitting and maximising the chance of being gerenalisable to future data. We can do this using cross-validation as below. Either look at the cross-validation complexity plot or use the output of the xerror (cross-validation error) variable. There are two strategies to choose the value of complexity parameter (and the corresponding tree size). The first option is to prune the tree back to the point where the cross-validated error is a minimum, known as the minimum error strategy. The second option is to prune the tree using the complexity parameter of the smallest tree that is within one standard deviation of the tree with the smallest xerror, known as the smallest tree strategy. In the case that there are multiple trees with the same xerror; we will choose the smaller one (since that tree would predict as well as the other one but it would also have fewer branches; thus we can also avoid overfitting). Task Build a fully grown tree. Hint Change the values of minsplit, minbucket, maxdepth and cp in rpart.control. The complexity parameter (cp) table for the fitted tree can be produced by using printcp. Given the output from a fully grown tree, decide the appropriate cp value using both the minimum error strategy and the smallest tree strategy. printcp(Full_tree) ## ## Classification tree: ## rpart(formula = sex ~ FL + RW + CL + CW + BD, data = train.crab, ## method = &quot;class&quot;, control = rpart.control(minsplit = 2, minbucket = 1, ## maxdepth = 30, cp = -1)) ## ## Variables actually used in tree construction: ## [1] BD CL FL RW ## ## Root node error: 49/100 = 0.49 ## ## n= 100 ## ## CP nsplit rel error xerror xstd ## 1 0.265306 0 1.000000 1.28571 0.098531 ## 2 0.173469 1 0.734694 0.87755 0.101036 ## 3 0.051020 4 0.204082 0.53061 0.089517 ## 4 0.030612 6 0.102041 0.48980 0.087160 ## 5 0.020408 8 0.040816 0.48980 0.087160 ## 6 0.010204 9 0.020408 0.42857 0.083124 ## 7 -1.000000 11 0.000000 0.42857 0.083124 For the following four questions, enter your answer by rounding to 3 decimal places. What is the training error rate after 4 splits? Hint The training error rate equals to rel error (relative training error rate) multiplied by root node error. What is the cross-validation error rate after 4 splits? Hint The cross-validation error rate equals to xerror multiplied by root node error. Which cp value you would choose when using the minimum error strategy? Which cp value you would choose when using the smallest tree strategy? We could also choose the cp value according to the cp plot. The minimum error strategy refers to the cp value which has the lowest X-val Relative Error. The smallest tree strategy refers to the largest cp value which is under the dashed line; the intercept of this line equals to the minimum xerror plus its standard deviation. plotcp(Full_tree) Prune the fully grown classification tree using the cp value found from the smallest tree strategy. Calculate sensitivity and specificity for training and validation sets again and compare to the results in Task 5. "],["exercise-2-titanic-data.html", "3 Exercise 2: Titanic Data", " 3 Exercise 2: Titanic Data The Titanic data can be downloaded from the titanic package in R. Once you do that, there will be two data sets loaded in R; a training data set, titanic_train, and a test data set, test_train. The dataset includes 12 variables, described as follows: PassengerID: Passenger ID Survived: Survival (0 = No; 1 = Yes) Pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) Name: Name Sex: Sex Age: Age SibSp: Number of Siblings/Spouses Aboard Parch: Number of Parents/Children Aboard Ticket: Ticket Number Fare: Passenger Fare (British pound) Cabin: Cabin Embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) Your task is to create a model that predicts who survives and who dies during the Titanic event using bagging and random forest, and compare between the two approaches. Task Perform some exploratory analysis on this data. In particular, understand the variable type, distribution of each variable, and identify which variables may be useful in predicting survival status. Hint Try str, skim, summary for numerical summaries and plot, pairs for graphical summaries. Split the training data titanic_train further into a training (80%) and validation (20%) set. This dataset includes missing data. Unlike classification trees which can construct the tree using surrogate splits, bagging and random forest cannot be directly applied to missing data. To fix it, we use the rough imputation method ,na.roughfix, provided in the randomForest package, which replaces missing values of continuous variables with their medians and missing values of categorical variables with their modes. More details can be found in the package documentation. library(randomForest) train.imputed &lt;- na.roughfix(train) #train: training data after training/validation split Apply bagging and random forest to the imputed training data (train.imputed). Which method would you prefer in this case? Hint Use the command randomForest for building a bagging and random forest classifier. Remember to change the argument mtry when applying bagging. Compare the two methods based on some appropriate evaluation criteria. Which variables are important in predicting survival under bagging and random forest? Hint Use the command varImpPlot or use ggplot as in Examples 11 and 12 in Week 4 lecture note. Use the built model to predict the test data titanic_test. Optional Task Build a classification tree on Titanic train and compare its result with Bagging and Random Forest. "],["solution.html", "4 Solution 4.1 Exercise 1 4.2 Exercise 2", " 4 Solution 4.1 Exercise 1 Task 1 set.seed(2023) n &lt;- nrow(crabs) ind1 &lt;- sample(c(1:n), round(n/2)) ind2 &lt;- sample(c(1:n)[-ind1], round(n/4)) ind3 &lt;- setdiff(c(1:n),c(ind1,ind2)) train.crab &lt;- data.crabs[ind1, ] valid.crab &lt;- data.crabs[ind2, ] test.crab &lt;- data.crabs[ind2, ] Task 2 library(rpart) library(rpart.plot) crabs.rt &lt;- rpart(sex~FL + RW + CL + CW + BD, data=train.crab, method=&quot;class&quot;) rpart.plot(crabs.rt,type=2,extra=4) Task 3 crabs.rt ## n= 100 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 100 49 M (0.49000000 0.51000000) ## 2) RW&gt;=15.9 15 1 F (0.93333333 0.06666667) * ## 3) RW&lt; 15.9 85 35 M (0.41176471 0.58823529) ## 6) CL&lt; 35.65 65 30 F (0.53846154 0.46153846) ## 12) RW&gt;=12.75 17 0 F (1.00000000 0.00000000) * ## 13) RW&lt; 12.75 48 18 M (0.37500000 0.62500000) ## 26) BD&lt; 11.95 27 9 F (0.66666667 0.33333333) * ## 27) BD&gt;=11.95 21 0 M (0.00000000 1.00000000) * ## 7) CL&gt;=35.65 20 0 M (0.00000000 1.00000000) * Task 5 To check if our model is overfitting, we could compare the classification performance on training and validation sets. # training performance train.pred &lt;- predict(crabs.rt, newdata=train.crab[,-1],type=&quot;class&quot;) table(train.crab[,1], train.pred) ## train.pred ## F M ## F 49 0 ## M 10 41 # validation performance valid.pred &lt;- predict(crabs.rt, newdata=valid.crab[,-1],type=&quot;class&quot;) table(valid.crab[,1], valid.pred) ## valid.pred ## F M ## F 20 3 ## M 10 17 Given this is a two-class classification problem, all evaluation measures introduced in Week 3 can be applied. In addition, considering that there are more female crabs being misclassified as male in the training data (\\(9\\) for F-&gt;M against \\(1\\) for M-&gt;F), it is preferable to use some measures such as class-specific correct classification rate, sensitivity and specificity, AUC to reflect this issue. train.table &lt;- table(train.crab[,1], train.pred) train.table[1,1]/sum(train.table[1,]) # training sensitivity ## [1] 1 train.table[2,2]/sum(train.table[2,]) # training specificity ## [1] 0.8039216 valid.table &lt;- table(valid.crab[,1], valid.pred) valid.table[1,1]/sum(valid.table[1,]) # validation sensitivity ## [1] 0.8695652 valid.table[2,2]/sum(valid.table[2,]) # validation specificity ## [1] 0.6296296 The training and validation sensitivity are roughly the same, while validation specificity is lower than training specificity. This could be an indication of overfitting. Task 6 Full_tree &lt;- rpart(sex~FL + RW + CL + CW + BD, data=train.crab, method=&quot;class&quot;, control=rpart.control(minsplit=2,minbucket=1,maxdepth=30,cp=-1)) Task 7 0.204082*0.49 0.53061*0.49 Here the lowest xerror is 0.42857 with xstd 0.083124. So, when using the minimum error strategy, we would choose the cp value which is larger than the one from Tree 6 and smaller than Tree 5, i.e. between 0.011 and 0.020. When using the smallest tree strategy, we would choose the cp value from the smallest tree whose xerror is smaller than 0.511694 (0.42857+0.083124). This corresponds to Tree 4, suggesting us to use a cp value between 0.031 and 0.051. Task 8 # prune the tree crabs.rt.pruned &lt;- prune(Full_tree, cp=0.04) rpart.plot(crabs.rt.pruned) # training performance train.pred &lt;- predict(crabs.rt, newdata=train.crab[,-1],type=&quot;class&quot;) train.table &lt;- table(train.crab[,1], train.pred) train.table ## train.pred ## F M ## F 49 0 ## M 10 41 train.table[1,1]/sum(train.table[1,]) # training sensitivity ## [1] 1 train.table[2,2]/sum(train.table[2,]) # training specificity ## [1] 0.8039216 # validation performance valid.pred &lt;- predict(crabs.rt, newdata=valid.crab[,-1],type=&quot;class&quot;) valid.table &lt;- table(valid.crab[,1], valid.pred) valid.table ## valid.pred ## F M ## F 20 3 ## M 10 17 valid.table[1,1]/sum(valid.table[1,]) # validation sensitivity ## [1] 0.8695652 valid.table[2,2]/sum(valid.table[2,]) # validation specificity ## [1] 0.6296296 While pruning strategy is used, the pruned tree appears more complicated than the previous tree. Therefore, there is a strong sign of over-fitting. 4.2 Exercise 2 Task 1 Let's use str first: library(titanic) str(titanic_train) ## &#39;data.frame&#39;: 891 obs. of 12 variables: ## $ PassengerId: int 1 2 3 4 5 6 7 8 9 10 ... ## $ Survived : int 0 1 1 1 0 0 0 0 1 1 ... ## $ Pclass : int 3 1 3 1 3 3 1 3 3 2 ... ## $ Name : chr &quot;Braund, Mr. Owen Harris&quot; &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot; &quot;Heikkinen, Miss. Laina&quot; &quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot; ... ## $ Sex : chr &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ Age : num 22 38 26 35 35 NA 54 2 27 14 ... ## $ SibSp : int 1 1 0 1 0 0 0 3 0 1 ... ## $ Parch : int 0 0 0 0 0 0 0 1 2 0 ... ## $ Ticket : chr &quot;A/5 21171&quot; &quot;PC 17599&quot; &quot;STON/O2. 3101282&quot; &quot;113803&quot; ... ## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ... ## $ Cabin : chr &quot;&quot; &quot;C85&quot; &quot;&quot; &quot;C123&quot; ... ## $ Embarked : chr &quot;S&quot; &quot;C&quot; &quot;S&quot; &quot;S&quot; ... We shouldn't be happy with some of the variable types, especially when we would have to use some of them for our analysis later on. For example, Name and Ticket are unique identifiers of each passenger, thus providing no predictive power; Fare and Cabin might be redundant given PClass. In addition, for our analysis, we should transform the variables Survived, Pclassand Sex into factors. train &lt;- titanic_train[,c(2,3,5,6,7)] train$Survived &lt;- factor(train$Survived, levels=c(0,1), labels=c(&quot;Died&quot;,&quot;Survived&quot;)) train$Pclass &lt;- as.factor(train$Pclass) train$Sex &lt;- as.factor(train$Sex) Let's start with some exploratory analysis of the data set: library(skimr);library(tibble) titanic &lt;- train %&gt;% as_tibble() my_skim &lt;- skim_with(base=sfl(n=length,n_missing=n_missing),factor=sfl(ordered=NULL), numeric=sfl(p0=NULL,p100=NULL,hist = NULL)) knit_print(my_skim(titanic)) Table 4.1: Data summary Name titanic Number of rows 891 Number of columns 5 _______________________ Column type frequency: factor 3 numeric 2 ________________________ Group variables None Variable type: factor skim_variable n n_missing n_unique top_counts Survived 891 0 2 Die: 549, Sur: 342 Pclass 891 0 3 3: 491, 1: 216, 2: 184 Sex 891 0 2 mal: 577, fem: 314 Variable type: numeric skim_variable n n_missing mean sd p25 p50 p75 Age 891 177 29.70 14.53 20.12 28 38 SibSp 891 0 0.52 1.10 0.00 0 1 Note that almost 20% of our data set has missing values in the variable Age. Let's also have a look at the relationship between survival and the passenger's class. library(ggplot2) ggplot(train, aes(Pclass, fill=Survived)) + geom_bar() + xlab(&quot;Passenger Class&quot;) + ylab(&quot;Number of Passengers&quot;) + ggtitle(&quot;Survival by Passenger Class&quot;) + scale_fill_discrete(name = &quot;&quot;, labels = c(&quot;Died&quot;, &quot;Survived&quot;)) + theme(plot.title = element_text(hjust = 0.5, size = 10)) It seems that passengers who were on the first class had a higher probability of surviving compared to the other classes (especially the 3rd one). Since we are a bit curious about this, let's be a bit more specific and find the specific probabilities using R. prop.table(table(train$Survived,train$Pclass),2) ## ## 1 2 3 ## Died 0.3703704 0.5271739 0.7576375 ## Survived 0.6296296 0.4728261 0.2423625 It seems that the probability of surviving as a first class passenger is more than twice the probability of surviving as a third class passenger (i.e. \\(63\\%\\) versus \\(24\\%\\)). Not a coincidence that the character Di Caprio played in Titanic, a third class passenger named Jack Dawson, did not survive the accident. Task 2 set.seed(1) n &lt;- nrow(titanic_train) idx &lt;- sample(1:n, round(0.2*n)) val &lt;- train[idx,] train &lt;- train[-idx,] Task 3 set.seed(1) bagging &lt;- randomForest(Survived~Pclass+Age+Sex+SibSp, data=train.imputed, mtry=4, ntree=200) set.seed(1) rf &lt;- randomForest(Survived~Pclass+Age+Sex+SibSp, data=train.imputed, ntree=200) To decide whether to use bagging or random forest, one way is to look at the out-of-bag estimate of error rate, which can be found by printing out the model output (i.e. type bagging and rf directly in R). However, as the class is imbalanced (i.e. the number of observations in each class is different), accuracy may not be the optimal evaluation measure. Here we will use AUC instead. val.imputed &lt;- na.roughfix(val) bagging_prob &lt;- predict(bagging, val.imputed, type=&quot;prob&quot;) rf_prob &lt;- predict(rf, val.imputed, type=&quot;prob&quot;) library(ROCR) bagging_pred &lt;- prediction(bagging_prob[,2], val$Survived) bagging_AUC &lt;- performance(bagging_pred, &quot;auc&quot;)@y.values[[1]] rf_pred &lt;- prediction(rf_prob[,2], val$Survived) rf_AUC &lt;- performance(rf_pred, &quot;auc&quot;)@y.values[[1]] print(c(bagging_AUC,rf_AUC)) ## [1] 0.8544942 0.8840409 It turns out random forest achieves the highest AUC. Task 4. Below is an example of visualising variable importance of random forest. varImpPlot(rf, main=&quot;Predicting survival in Titanic data&quot;) # ggplot for variable importance # library(forcats);library(caret);library(dplyr) # rf_df &lt;- data_frame(var = rownames(randomForest::importance(rf)), # MeanDecreaseGini = randomForest::importance(rf)[,1]) %&gt;% # mutate(var = fct_reorder(var, MeanDecreaseGini, median)) # rf_ggplot &lt;- ggplot(rf_df,aes(var, MeanDecreaseGini)) + # geom_point() + coord_flip() + # labs(title = &quot;Predicting survival in Titanic data&quot;, # x = NULL,y = &quot;Average decrease in the Gini Index&quot;) + # theme(plot.title = element_text(hjust = 0.5)) # rf_ggplot According to the random forest model, sex is the most important factor, followed by age and passenger. The number of siblings or spouses have little importance in survival. Task 5 test &lt;- titanic_test[,c(2,4,5,6)] test$Pclass &lt;- as.factor(test$Pclass) test$Sex &lt;- as.factor(test$Sex) test.imputed &lt;- na.roughfix(test) test.rf &lt;- predict(rf, test.imputed, type=&quot;class&quot;) The data set comes from the Kaggle competition, which does not provide the test labels to the public. Therefore we cannot further evaluate the test performance. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
